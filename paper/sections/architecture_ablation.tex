% Architecture Ablation Study - Comprehensive Results
% Full comparison of encoder architectures on UP-FALL and WEDA-FALL

%-------------------------------------------------------------------
% ARCHITECTURE COMPARISON ACROSS DATASETS
%-------------------------------------------------------------------
\subsection{Encoder Architecture Ablation}
\label{subsec:encoder_ablation}

To evaluate the generality of our findings, we conduct a comprehensive architecture ablation comparing four encoder types---Transformer, CNN, LSTM, and Mamba---across UP-FALL and WEDA-FALL datasets. Each architecture is evaluated with both Kalman-fused (7ch) and raw (6ch) input, yielding 20 experimental configurations. All models use dual-stream structure with embed\_dim=48 and are evaluated using full LOSO cross-validation (15 folds for UP-FALL, 12 folds for WEDA-FALL).

%-------------------------------------------------------------------
% UP-FALL COMPLETE RESULTS
%-------------------------------------------------------------------
\subsubsection{UP-FALL Results}
\label{subsubsec:upfall_results}

Table~\ref{tab:upfall_complete} presents complete metrics for all architectures on UP-FALL.

\begin{table}[H]
\caption{Complete architecture comparison on UP-FALL (15-fold LOSO-CV). All metrics as mean $\pm$ std. Best result in \textbf{bold}.}
\label{tab:upfall_complete}
\centering
\small
\begin{tabular}{llcccccc}
\toprule
\textbf{Architecture} & \textbf{Input} & \textbf{Test F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} & \textbf{Loss ($\times 10^{-4}$)} \\
\midrule
\multirow{2}{*}{Trans-Dual}
 & Kalman & \textbf{94.43 $\pm$ 4.60} & 95.78 $\pm$ 3.75 & 93.44 $\pm$ 8.08 & \textbf{96.09 $\pm$ 5.09} & 98.87 $\pm$ 1.08 & 4.29 \\
 & Raw & 94.17 $\pm$ 4.42 & \textbf{95.90 $\pm$ 3.04} & \textbf{94.73 $\pm$ 5.46} & 94.02 $\pm$ 6.59 & 98.81 $\pm$ 1.41 & 5.97 \\
\midrule
\multirow{2}{*}{Trans-Single}
 & Kalman & 93.17 $\pm$ 4.94 & 95.11 $\pm$ 3.42 & 93.51 $\pm$ 6.51 & 93.50 $\pm$ 7.60 & \textbf{98.86 $\pm$ 1.14} & 4.70 \\
 & Raw & 91.06 $\pm$ 8.44 & 93.64 $\pm$ 6.15 & 91.90 $\pm$ 9.69 & 91.44 $\pm$ 11.08 & 98.09 $\pm$ 2.46 & 7.48 \\
\midrule
\multirow{2}{*}{CNN}
 & Kalman & 89.58 $\pm$ 9.71 & 92.46 $\pm$ 6.71 & 89.59 $\pm$ 10.92 & 90.59 $\pm$ 12.08 & 96.79 $\pm$ 5.40 & 5.17 \\
 & Raw & 93.00 $\pm$ 7.78 & 95.24 $\pm$ 4.96 & 94.22 $\pm$ 6.85 & 92.74 $\pm$ 11.65 & 99.00 $\pm$ 2.15 & \textbf{4.05} \\
\midrule
\multirow{2}{*}{LSTM}
 & Kalman & 82.53 $\pm$ 17.40 & 87.92 $\pm$ 9.23 & 85.07 $\pm$ 14.94 & 86.07 $\pm$ 20.39 & 95.55 $\pm$ 4.34 & 9.59 \\
 & Raw & 91.50 $\pm$ 8.56 & 94.32 $\pm$ 4.88 & 93.47 $\pm$ 6.05 & 90.60 $\pm$ 12.67 & 98.59 $\pm$ 2.66 & 4.76 \\
\midrule
\multirow{2}{*}{Mamba}
 & Kalman & 91.61 $\pm$ 10.41 & 94.47 $\pm$ 6.01 & 94.20 $\pm$ 8.52 & 90.05 $\pm$ 13.79 & 97.61 $\pm$ 4.84 & 4.72 \\
 & Raw & 92.34 $\pm$ 11.06 & 95.43 $\pm$ 5.25 & 97.93 $\pm$ 2.40 & 89.15 $\pm$ 15.76 & 98.63 $\pm$ 3.62 & 7.04 \\
\bottomrule
\end{tabular}
\end{table}

On UP-FALL, the dual-stream Kalman transformer achieves the highest F1 score (94.43\%), with Kalman fusion providing a modest +0.26\% improvement over raw input. The transformer architecture consistently outperforms alternatives, with trans-single-kalman (93.17\%) also exceeding CNN and LSTM variants.

%-------------------------------------------------------------------
% WEDA-FALL COMPLETE RESULTS
%-------------------------------------------------------------------
\subsubsection{WEDA-FALL Results}
\label{subsubsec:wedafall_results}

Table~\ref{tab:wedafall_complete} presents complete metrics for WEDA-FALL.

\begin{table}[H]
\caption{Complete architecture comparison on WEDA-FALL (12-fold LOSO-CV). All metrics as mean $\pm$ std. Best result in \textbf{bold}.}
\label{tab:wedafall_complete}
\centering
\small
\begin{tabular}{llcccccc}
\toprule
\textbf{Architecture} & \textbf{Input} & \textbf{Test F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{AUC} & \textbf{Loss ($\times 10^{-4}$)} \\
\midrule
\multirow{2}{*}{Trans-Dual}
 & Kalman & 91.49 $\pm$ 4.32 & 88.82 $\pm$ 6.00 & 87.41 $\pm$ 6.07 & 96.19 $\pm$ 3.76 & 95.76 $\pm$ 6.22 & 4.45 \\
 & Raw & 90.43 $\pm$ 2.63 & 87.25 $\pm$ 3.93 & 84.92 $\pm$ 5.43 & 97.10 $\pm$ 3.23 & 94.52 $\pm$ 4.58 & 6.57 \\
\midrule
\multirow{2}{*}{Trans-Single}
 & Kalman & 90.27 $\pm$ 8.19 & 87.92 $\pm$ 8.84 & 86.96 $\pm$ 5.42 & 94.42 $\pm$ 11.73 & 95.80 $\pm$ 6.84 & 5.12 \\
 & Raw & 91.92 $\pm$ 3.07 & 89.45 $\pm$ 4.45 & 88.37 $\pm$ 4.88 & 96.03 $\pm$ 4.01 & 95.26 $\pm$ 4.77 & 5.34 \\
\midrule
\multirow{2}{*}{CNN}
 & Kalman & 91.17 $\pm$ 2.79 & 88.45 $\pm$ 3.72 & 86.31 $\pm$ 3.85 & 96.77 $\pm$ 3.41 & 96.42 $\pm$ 3.69 & 4.38 \\
 & Raw & 93.27 $\pm$ 2.37 & 91.27 $\pm$ 3.16 & 89.24 $\pm$ 2.60 & \textbf{97.75 $\pm$ 3.11} & 97.40 $\pm$ 2.43 & \textbf{3.69} \\
\midrule
\multirow{2}{*}{LSTM}
 & Kalman & 90.22 $\pm$ 6.24 & 87.58 $\pm$ 7.27 & 87.37 $\pm$ 5.57 & 93.82 $\pm$ 9.23 & 94.97 $\pm$ 6.17 & 5.46 \\
 & Raw & \textbf{94.01 $\pm$ 3.39} & \textbf{92.43 $\pm$ 4.17} & \textbf{91.72 $\pm$ 4.31} & 96.70 $\pm$ 5.19 & \textbf{97.70 $\pm$ 2.71} & 4.15 \\
\midrule
\multirow{2}{*}{Mamba}
 & Kalman & 91.09 $\pm$ 5.79 & 88.36 $\pm$ 8.07 & 88.03 $\pm$ 7.46 & 94.65 $\pm$ 5.59 & 94.27 $\pm$ 10.85 & 6.06 \\
 & Raw & 92.87 $\pm$ 3.16 & 90.61 $\pm$ 4.52 & 88.97 $\pm$ 5.73 & 97.45 $\pm$ 3.06 & 97.07 $\pm$ 2.49 & 4.38 \\
\bottomrule
\end{tabular}
\end{table}

On WEDA-FALL, LSTM-Raw achieves the highest F1 (94.01\%), outperforming all transformer variants. This result contrasts with UP-FALL, where transformers dominate. The performance divergence is analyzed in Section~\ref{subsubsec:dataset_analysis}.

%-------------------------------------------------------------------
% TRAIN/VAL/TEST ANALYSIS
%-------------------------------------------------------------------
\subsubsection{Train-Validation-Test Analysis}
\label{subsubsec:train_val_test}

Table~\ref{tab:train_val_test_all} presents training dynamics and generalization gaps for the top-performing configurations.

\begin{table}[H]
\caption{Train/Val/Test F1 scores for top architectures. $\Delta$ = Val F1 $-$ Test F1 (generalization gap).}
\label{tab:train_val_test_all}
\centering
\begin{tabular}{llccccc}
\toprule
\textbf{Dataset} & \textbf{Architecture} & \textbf{Val F1} & \textbf{Test F1} & \textbf{$\Delta$F1} & \textbf{Macro-F1} \\
\midrule
\multirow{5}{*}{UP-FALL}
 & Trans-Dual-Kalman & 93.88 & 94.43 & $-$0.55 & 95.49 \\
 & Trans-Dual-Raw & 95.85 & 94.17 & +1.68 & 95.50 \\
 & Trans-Single-Kalman & 93.75 & 93.17 & +0.58 & 94.66 \\
 & CNN-Raw & 96.66 & 93.00 & +3.66 & 94.69 \\
 & Mamba-Raw & 97.47 & 92.34 & +5.13 & 94.51 \\
\midrule
\multirow{5}{*}{WEDA-FALL}
 & LSTM-Raw & 97.10 & 94.01 & +3.09 & 91.76 \\
 & CNN-Raw & 96.66 & 93.27 & +3.39 & 90.35 \\
 & Mamba-Raw & 97.07 & 92.87 & +4.20 & 89.42 \\
 & Trans-Single-Raw & 95.69 & 91.92 & +3.77 & 88.13 \\
 & Trans-Dual-Kalman & 94.78 & 91.49 & +3.29 & 87.47 \\
\bottomrule
\end{tabular}
\end{table}

Key observations: (1) UP-FALL Trans-Dual-Kalman shows \textit{negative} generalization gap, indicating test performance exceeds validation---validation subjects may be harder cases; (2) WEDA-FALL exhibits consistently larger gaps (3--4\%), reflecting higher inter-subject variability in consumer sensor data; (3) Macro-F1 scores reveal class balance differences---UP-FALL macro-F1 closely tracks F1, while WEDA-FALL shows 2--4\% reduction, indicating more class imbalance.

%-------------------------------------------------------------------
% KALMAN VS RAW COMPREHENSIVE
%-------------------------------------------------------------------
\subsection{Kalman Fusion Effect by Architecture}
\label{subsec:kalman_effect}

Table~\ref{tab:kalman_delta_all} quantifies the Kalman fusion benefit ($\Delta$F1 = Kalman F1 $-$ Raw F1) for each architecture.

\begin{table}[H]
\caption{Kalman vs Raw F1 comparison. Positive $\Delta$F1 indicates Kalman improves over Raw.}
\label{tab:kalman_delta_all}
\centering
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{UP-FALL}} & \multicolumn{3}{c}{\textbf{WEDA-FALL}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Architecture} & \textbf{Kalman} & \textbf{Raw} & \textbf{$\Delta$F1} & \textbf{Kalman} & \textbf{Raw} & \textbf{$\Delta$F1} \\
\midrule
Trans-Dual & 94.43 & 94.17 & \textbf{+0.26} & 91.49 & 90.43 & \textbf{+1.06} \\
Trans-Single & 93.17 & 91.06 & \textbf{+2.11} & 90.27 & 91.92 & $-$1.65 \\
CNN & 89.58 & 93.00 & $-$3.42 & 91.17 & 93.27 & $-$2.10 \\
LSTM & 82.53 & 91.50 & $-$8.97 & 90.22 & 94.01 & $-$3.79 \\
Mamba & 91.61 & 92.34 & $-$0.73 & 91.09 & 92.87 & $-$1.78 \\
\bottomrule
\end{tabular}
\end{table}

A clear pattern emerges: \textbf{only the dual-stream transformer consistently benefits from Kalman fusion across both datasets}. CNN, LSTM, and Mamba show degraded performance with Kalman input, with the effect most pronounced for LSTM (UP-FALL: $-$8.97\%, WEDA-FALL: $-$3.79\%). We attribute this to implicit temporal smoothing in these architectures conflicting with Kalman preprocessing.

%-------------------------------------------------------------------
% PER-FOLD ANALYSIS
%-------------------------------------------------------------------
\subsection{Per-Fold Performance Distribution}
\label{subsec:per_fold}

Figure~\ref{fig:fold_distribution} visualizes per-fold F1 distributions for top architectures.

\begin{figure}[H]
\centering
\includegraphics[width=0.48\textwidth]{figures/upfall_bar_chart.png}
\includegraphics[width=0.48\textwidth]{figures/wedafall_bar_chart.png}
\caption{\textbf{Architecture comparison across datasets.} Left: UP-FALL (15 folds). Right: WEDA-FALL (12 folds). Error bars show standard deviation across folds.}
\label{fig:fold_distribution}
\end{figure}

%-------------------------------------------------------------------
% BEST AND WORST FOLD ANALYSIS
%-------------------------------------------------------------------
\subsubsection{Best and Worst Fold Analysis}
\label{subsubsec:fold_extremes}

Table~\ref{tab:fold_extremes} identifies the best and worst performing folds for each dataset's top configuration.

\begin{table}[H]
\caption{Best and worst fold performance for top configurations.}
\label{tab:fold_extremes}
\centering
\begin{tabular}{llcccc}
\toprule
\textbf{Dataset} & \textbf{Architecture} & \textbf{Best Fold F1} & \textbf{Worst Fold F1} & \textbf{Range} & \textbf{IQR} \\
\midrule
UP-FALL & Trans-Dual-Kalman & 100.0\% & 85.32\% & 14.68\% & 6.15\% \\
UP-FALL & Trans-Dual-Raw & 100.0\% & 86.94\% & 13.06\% & 5.89\% \\
WEDA-FALL & LSTM-Raw & 98.25\% & 85.10\% & 13.15\% & 4.14\% \\
WEDA-FALL & Trans-Dual-Kalman & 95.54\% & 80.37\% & 15.17\% & 5.62\% \\
\bottomrule
\end{tabular}
\end{table}

UP-FALL Trans-Dual-Kalman achieves perfect F1 (100\%) on fold 11, while the worst fold (fold 9) reaches 85.32\%. WEDA-FALL shows similar variance, with LSTM-Raw ranging from 85.10\% to 98.25\%. The narrower interquartile range (IQR) for LSTM-Raw (4.14\%) compared to Trans-Dual-Kalman (5.62\%) indicates more consistent generalization.

%-------------------------------------------------------------------
% DATASET-SPECIFIC ANALYSIS
%-------------------------------------------------------------------
\subsection{Dataset-Dependent Architecture Selection}
\label{subsubsec:dataset_analysis}

The divergent results between UP-FALL and WEDA-FALL highlight the importance of dataset-architecture co-design. Three factors explain the performance differences:

\begin{enumerate}
    \item \textbf{Dataset size}: WEDA-FALL contains only 14 subjects (12 test folds), compared to UP-FALL's 17 subjects (15 folds). Transformers require more data to learn effective attention patterns, while LSTMs benefit from stronger inductive biases for sequential data.

    \item \textbf{Model stability}: Standard deviation analysis reveals transformers exhibit higher variance on smaller datasets. Trans-Single-Kalman shows $\sigma = 8.19\%$ on WEDA-FALL versus LSTM-Raw's $\sigma = 3.39\%$, indicating less stable generalization.

    \item \textbf{Sensor characteristics}: WEDA-FALL uses consumer-grade Fitbit (50~Hz) while UP-FALL uses research-grade IMU (18~Hz). Consumer sensors may benefit more from LSTM's implicit noise handling than from explicit Kalman filtering, which was originally tuned for different sensor characteristics.
\end{enumerate}

%-------------------------------------------------------------------
% PRECISION-RECALL TRADEOFFS
%-------------------------------------------------------------------
\subsection{Precision-Recall Trade-offs}
\label{subsec:precision_recall}

Table~\ref{tab:pr_analysis} examines precision-recall balance across top configurations.

\begin{table}[H]
\caption{Precision-Recall analysis for top configurations on WEDA-FALL.}
\label{tab:pr_analysis}
\centering
\begin{tabular}{llcccc}
\toprule
\textbf{Architecture} & \textbf{Input} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Operating Point} \\
\midrule
LSTM & Raw & 91.72 $\pm$ 4.31 & 96.70 $\pm$ 5.19 & \textbf{94.01} & Balanced-optimal \\
CNN & Raw & 89.24 $\pm$ 2.60 & 97.75 $\pm$ 3.11 & 93.27 & High recall \\
Trans-Dual & Kalman & 87.41 $\pm$ 6.07 & 96.19 $\pm$ 3.76 & 91.49 & Recall-oriented \\
Trans-Dual & Raw & 84.92 $\pm$ 5.43 & 97.10 $\pm$ 3.23 & 90.43 & Very high recall \\
\bottomrule
\end{tabular}
\end{table}

LSTM-Raw achieves the optimal precision-recall balance (91.72\%/96.70\%), explaining its F1 superiority. Transformer variants show systematically lower precision but comparable recall, suggesting they produce more false positives on the smaller WEDA-FALL dataset.

%-------------------------------------------------------------------
% COMPUTATIONAL COST
%-------------------------------------------------------------------
\subsection{Computational Cost}
\label{subsec:compute_cost}

Table~\ref{tab:compute_cost} reports training time for each configuration.

\begin{table}[H]
\caption{Training time per experiment (full LOSO-CV, 4 GPUs).}
\label{tab:compute_cost}
\centering
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{UP-FALL (15 folds)}} & \multicolumn{2}{c}{\textbf{WEDA-FALL (12 folds)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Architecture} & \textbf{Kalman} & \textbf{Raw} & \textbf{Kalman} & \textbf{Raw} \\
\midrule
Trans-Dual & 823s & 515s & 293s & 435s \\
Trans-Single & 258s & 359s & 915s & 408s \\
CNN & 249s & 476s & 305s & 529s \\
LSTM & 232s & 453s & 289s & 366s \\
Mamba & 316s & 482s & 1052s & 509s \\
\bottomrule
\end{tabular}
\end{table}

LSTM provides the best accuracy-efficiency trade-off on WEDA-FALL: highest F1 (94.01\%) with moderate training time (366s). Mamba shows unexpectedly high variance in training time, likely due to state-space dimension tuning overhead.

%-------------------------------------------------------------------
% CROSS-DATASET COMPARISON FIGURE
%-------------------------------------------------------------------
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/cross_dataset_comparison.png}
\caption{\textbf{Cross-dataset architecture comparison.} Trans-Dual-Kalman performs best on UP-FALL (research-grade sensor, larger dataset), while LSTM-Raw dominates WEDA-FALL (consumer sensor, smaller dataset). This highlights the need for dataset-specific architecture selection.}
\label{fig:cross_dataset}
\end{figure}

%-------------------------------------------------------------------
% RECOMMENDATIONS
%-------------------------------------------------------------------
\subsection{Architecture Selection Guidelines}
\label{subsec:guidelines}

Based on our comprehensive ablation:

\begin{enumerate}
    \item \textbf{Large datasets ($\geq$15 subjects), research-grade sensors}: Dual-stream Kalman transformer recommended. Achieves highest F1 with robust orientation features.

    \item \textbf{Small datasets ($<$15 subjects), any sensor}: LSTM or CNN with raw input preferred. Stronger inductive biases provide more stable generalization.

    \item \textbf{Consumer-grade sensors}: Raw input often outperforms Kalman for non-transformer architectures. Implicit temporal smoothing in LSTM/CNN may conflict with explicit Kalman filtering.

    \item \textbf{Deployment constraints}: CNN provides competitive accuracy with lowest computational cost. LSTM offers best accuracy-efficiency trade-off.
\end{enumerate}

These findings underscore that optimal fall detection systems require dataset-architecture co-design, rather than assuming a single architecture generalizes across all sensor types and population sizes.
