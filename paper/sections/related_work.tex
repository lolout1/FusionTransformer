%
%
%
Fall detection using wearable IMUs has evolved through several methodological paradigms, each addressing specific limitations while introducing new challenges that motivate our proposed approach.

\noindent \textbf{Deep Learning Approaches for IMU-Based Fall Detection:} 
Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have dominated early deep learning approaches for fall detection. Hybrid CNN-based long short-term memory (CNN-LSTM) architectures exploit both spatial feature extraction capabilities of CNNs and temporal sequence modeling strengths of LSTMs, with a recent wrist-worn sensor study, presented by Hu et al.~\cite{HU2025103178}, achieving 96.94\% detection accuracy, 98.33\% sensitivity, and 96.67\% specificity on involuntary fall detection. Similarly, Luna-Perej√≥n et al.~\cite{LunaPerejon2019} demonstrated RNN-based models achieving 88.2\% sensitivity and 96.4\% specificity with real-time inference times below 34 ms on low-power microcontrollers. Musci et al.~\cite{Musci_2021} further validated online fall detection using LSTM blocks, comparing their performance against threshold-based approaches. However, these CNN-LSTM and RNN-based models face fundamental architectural limitations: LSTMs exhibit difficulty capturing long-range temporal dependencies due to gradient vanishing during backpropagation, and the sequential nature of recurrent processing precludes parallel computation across timesteps, increasing inference latency. 

To address the limitations of temporal dependency in unidirectional RNNs, bidirectional recurrent architectures (BiRNN) have recently been proposed. For example, a hybrid BiLSTM-Bi-Gated Recurrent Unit (GRU) GRU model~\cite{Ahirwar2025} with an additive attention mechanism, optimized using Binary Arithmetic Optimization Algorithm (BAOA) for feature selection, achieved state-of-the-art performance. BiGRU offers computational efficiency with reduced complexity and faster convergence. However, these models still suffer from sequential processing constraints and limited ability to model very long-range dependencies compared to attention-based mechanisms. 

In contrast, transformer-based approaches~\cite{vaswani2017} have emerged as a promising alternative to RNNs, leveraging self-attention mechanisms to capture both local and global temporal dependencies in parallel. A recent study presented by Zafar et al.~\cite{Zafar2025} achieved over 98\% accuracy using sliding window segmentation with majority voting and predictive smoothing to reduce false positives. Yhdego et al.~\cite{Yhdego2021} proposed a transformer attention network with Time2Vec positional encoding for gait analysis-based fall detection, demonstrating the effectiveness of attention mechanisms for temporal pattern recognition in IMU signals. The authors acknowledged that activities involving similar vertical and transitional movements (such as sitting down and standing up) remain indistinguishable in torso-based IMU data. Most critically, these single-stream designs concatenate heterogeneous sensor modalities at the input level, allowing noisy gyroscope measurements to corrupt accelerometer features during early convolutional processing, which is the failure mode we explicitly address through architectural decoupling, i.e., processing accelerometer and gyroscope data through separate, independent layers before fusion.

\noindent \textbf{Dual-Stream Architectures and Multi-Modal Fusion:} 
The recognition that different sensor modalities require separate processing pathways has motivated the development of dual-stream architectures. For example, the dual-stream convolutional neural network self-attention (DSCS) model~\cite{dscs2024} processes accelerometer and gyroscope data through separate convolutional streams before applying self-attention mechanisms to assign different weights to feature vectors, achieving outstanding performance. However, the architecture processes raw gyroscope angular velocities directly through convolutional layers without addressing the inherent noise and drift characteristics of consumer-grade MEMS sensors. Furthermore, the evaluation protocol did not employ Leave-One-Subject-Out cross-validation, potentially inflating reported performance through subject-specific pattern leakage across training and test splits.

A three-stream spatio-temporal graph convolutional network (GCN) for fall recognition~\cite{Shin2025} demonstrated the benefits of processing multiple feature representations through separate pathways. In multimodal sensing scenarios, decision-level fusion architectures consisting of distinct processing streams, such as vision-based streams exploiting skeletal landmarks and inertial-based streams using LSTM autoencoders, have shown that late fusion is more robust than early fusion by enabling better calibration, fault isolation, and resilience against modality-specific failures~\cite{Rehouma2025}. However, these approaches typically combine fundamentally different sensing modalities (vision and inertial) rather than addressing the specific challenge of fusing complementary measurements from the same IMU device.

The common thread across these dual-stream approaches is their failure to address data quality at the signal level: they assume that architectural separation alone suffices to handle noisy gyroscope measurements, ignoring the opportunity to transform these signals into more stable, semantically meaningful representations before feature extraction.

\noindent \textbf{Sensor Fusion and Kalman Filtering for Orientation Estimation:} 
Kalman filtering~\cite{kalman1960new} has been widely adopted in fall detection systems primarily for noise suppression and signal smoothing of accelerometer data. Liu and Lin~\cite{LIU2026109304} applied a first-order Kalman filter to extract slow-varying residual components from triaxial accelerometer signals, achieving 96.21\% accuracy and 93.24\% F1-score on using a support vector machine (SVM) with handcrafted features. Similarly, in~\cite{Shi2023Fusion}, complementary filtering techniques, such as the Madgwick algorithm, have been employed to fuse accelerometer gravity references with gyroscope angular velocities for orientation estimation in motion tracking applications. However, these approaches employed filtering solely for single-modality noise reduction or as preprocessing steps for threshold-based detection algorithms. Critically, no prior work has integrated multi-sensor Kalman fusion into dual-stream deep learning architectures, i.e., whether transforming noisy gyroscope data into stable orientation angles and processing them through separate neural pathways improves fall classification performance.


\noindent \textbf{Attention Mechanisms for Time-Series Classification:} 
Squeeze-and-Excitation (SE) networks~\cite{8652287, hu2018squeeze} and temporal attention mechanisms~\cite{ESSA2023110867} have demonstrated effectiveness in dynamically allocating weights to the features in human activity recognition (HAR). SE mechanisms perform channel-wise recalibration to amplify discriminative channels while suppressing less informative ones. Building on this principle, the study presented in~\cite{Wang2025} has shown that temporal attention can capture long-term dependencies without RNNs by integrating dilated CNNs with modified temporal attention mechanisms. These networks have further demonstrated that jointly modeling spatial and temporal dependencies improves HAR performance. However, the application of combined SE and temporal attention mechanisms to IMU-based fall detection remains underexplored. Critically, no prior work has examined whether attention mechanisms designed for single-stream architectures transfer effectively to dual-stream designs, where modality-specific fusion introduces different channel dynamics.