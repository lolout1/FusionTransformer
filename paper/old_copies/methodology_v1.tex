% Materials and Methods - BDCC Format
% Structure: Dataset → Sensor Fusion Strategy → Dual-Stream Architecture → Training → Evaluation
% Written in top ML conference style (NeurIPS/ICML)

%-------------------------------------------------------------------
% 2.1 DATASET
%-------------------------------------------------------------------
\subsection{Dataset}
\label{subsec:dataset}

We evaluate on the SmartFallMM dataset~\cite{ngu2022personalized}, comprising 51 participants (30 young adults ages 18--35, 21 older adults ages 65+) performing 14 activities: 9 ADLs (drinking water, picking up objects, putting on jacket, sweeping, hand washing, waving, walking, sitting, standing) and 5 fall types (forward, backward, left, right, rotational). Commodity smartwatches capture triaxial accelerometer and gyroscope data at 30~Hz.

\subsubsection{Subject Selection and Data Splits}

Table~\ref{tab:subjects} presents our subject selection criteria. \textbf{Training uses data from all available subjects} (both young and older adults) to maximize training diversity. However, \textbf{validation and testing are restricted to young subjects} because older adults did not perform simulated falls for safety reasons---their data contains only ADL samples. This design enables evaluation of fall detection generalization while leveraging older adult ADL patterns during training.

\begin{table}[H]
\caption{Subject selection for evaluation. Training includes all subjects; validation/test restricted to young adults (ages 18--35) who performed both ADLs and falls.}
\label{tab:subjects}
\centering
\small
\begin{tabular}{llcc}
\toprule
\textbf{Role} & \textbf{Population} & \textbf{Subject IDs} & \textbf{Count} \\
\midrule
Training & Young + Older & All available & 51 \\
Test (LOSO-CV rotation) & Young only & 31, 34, 36--38, 43--44, 46, 49--56, 58, 60--63 & 21 \\
Validation (fixed) & Young only & 48, 57 & 2 \\
Excluded (incomplete data) & Young & 29, 30, 32, 35, 39, 45, 59 & 7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Why older adults are excluded from test/validation.} The older adult subset (21 subjects, ages 65+) contains only ADL recordings---no simulated falls were collected due to safety and ethical considerations. Falls appear only in the young subset; older subjects contribute additional ADL variability to training. Including older adults in test sets would artificially inflate specificity (correct ADL classification) without measuring fall detection sensitivity. Future work will address cross-population generalization through domain adaptation techniques.

\textbf{Validation protocol clarification.} The fixed validation subjects (48, 57) are used \textit{exclusively} for early stopping during training. All hyperparameters and architectural decisions (embedding dimensions, dropout rates, attention mechanisms, Kalman filter parameters) were selected once prior to the main evaluation and held constant across all ablation experiments. This design follows best practices for avoiding selection bias in model evaluation~\cite{cawley2010overfitting}.

\textbf{Validation subject selection rationale.} Subjects 48 and 57 were chosen based on data quality (complete recordings across all activities) and are excluded from all training folds. To verify robustness to validation choice, we conducted a sensitivity analysis using alternative validation pairs (subjects 46/55 and 50/54); results varied by $<$0.3\% F1, confirming that our findings are not driven by the specific validation subjects.

\subsubsection{Data Segmentation}

We segment continuous sensor streams into fixed-length windows for classification:

\begin{itemize}
    \item \textbf{Window size:} 128 frames ($\approx$4.27 seconds at 30 Hz)
    \item \textbf{Stride:} Class-aware---64 frames for ADLs, 16 frames for falls
    \item \textbf{Rationale:} Smaller fall stride captures more training examples from the minority class, partially addressing class imbalance before loss-level weighting
\end{itemize}

The class-aware stride yields approximately 3,200 ADL windows and 1,800 fall windows per fold, with Focal Loss providing additional imbalance handling during training. Note that class-aware striding creates overlapping windows (especially for falls with a stride of size 16), introducing correlation between samples. Our statistical analysis operates on fold-level scores (one F1 per subject), ensuring independence between test samples for valid confidence intervals.

%-------------------------------------------------------------------
% 2.2 SENSOR FUSION STRATEGY
%-------------------------------------------------------------------
\subsection{Sensor Fusion Strategy}
\label{subsec:fusion}

We first present our sensor fusion approach, which transforms raw inertial measurements into semantically meaningful orientation features. This preprocessing step is critical: it determines what information the neural network receives.

\subsubsection{Motivation: The Gyroscope Problem}

A natural approach to IMU-based fall detection is concatenating accelerometer and gyroscope channels as input to a neural network. However, we observe that naively adding raw gyroscope data \textit{degrades} performance by 1.6--2.0\% F1. Analysis reveals two causes:

\begin{enumerate}
    \item \textbf{Noise contamination}: Consumer MEMS gyroscopes exhibit substantial measurement noise, with many subjects showing signal-to-noise ratios below unity during quiet standing. When processed through shared convolutional layers, this noise corrupts discriminative accelerometer features.
    \item \textbf{Semantic ambiguity}: Raw angular velocity ($\omega_x, \omega_y, \omega_z$) lacks absolute reference---the same rotation rate can occur in vastly different body orientations.
\end{enumerate}

Rather than discarding gyroscope data, we propose \textit{fusing} both sensors to estimate body orientation---a derived quantity that is both stable and semantically meaningful. Figure~\ref{fig:preprocessing} illustrates our preprocessing pipeline.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{preprocessing_pipeline.png}
\caption{\textbf{Preprocessing pipeline for IMU-based fall detection.} Raw triaxial accelerometer ($a_x, a_y, a_z$) and gyroscope ($\omega_x, \omega_y, \omega_z$) signals from a smartwatch are processed through a Linear Kalman Filter. The filter fuses both modalities to produce stable orientation estimates (roll $\phi$, pitch $\theta$, yaw $\psi$) while filtering gyroscope noise and correcting drift. The final 7-channel representation [SMV, $a_x$, $a_y$, $a_z$, $\phi$, $\theta$, $\psi$] combines impact dynamics (acceleration) with body pose information (orientation), enabling the neural network to learn both \textit{how hard} and \textit{in what direction} a fall occurs.}
\label{fig:preprocessing}
\end{figure}

\subsubsection{Linear Kalman Filter for Orientation Estimation}

We employ a Linear Kalman Filter to fuse accelerometer and gyroscope measurements into orientation angles. The key insight is that each sensor provides complementary information:

\begin{itemize}
    \item \textbf{Accelerometer}: Measures gravity direction, providing absolute roll/pitch reference when stationary, but corrupted by body acceleration during motion.
    \item \textbf{Gyroscope}: Measures angular velocity with high temporal resolution, enabling responsive motion tracking, but accumulates drift over time.
\end{itemize}

The Kalman filter optimally combines these signals: using accelerometer for drift correction and gyroscope for dynamic tracking.

\textbf{State Representation.} We model orientation as a 6D state vector:
\begin{equation}
\mathbf{x} = [\phi, \theta, \psi, \dot{\phi}, \dot{\theta}, \dot{\psi}]^{\T}
\end{equation}
where $\phi$ (roll), $\theta$ (pitch), $\psi$ (yaw) are Euler angles in radians, and $\dot{\phi}, \dot{\theta}, \dot{\psi}$ are angular rates.

\textbf{Predict Step.} At each timestep, we propagate the state using constant-velocity kinematics:
\begin{equation}
\hat{\mathbf{x}}_{k|k-1} = \mathbf{F} \hat{\mathbf{x}}_{k-1|k-1}, \quad \mathbf{F} = \begin{bmatrix} \mathbf{I}_3 & \Delta t \cdot \mathbf{I}_3 \\ \mathbf{0}_3 & \mathbf{I}_3 \end{bmatrix}
\end{equation}
This encodes the physical relationship: new angle = old angle + angular rate $\times$ time.

\textbf{Update Step.} We correct the prediction using sensor measurements:
\begin{equation}
\hat{\mathbf{x}}_{k|k} = \hat{\mathbf{x}}_{k|k-1} + \mathbf{K}_k (\mathbf{z}_k - \mathbf{H}\hat{\mathbf{x}}_{k|k-1})
\end{equation}
where $\mathbf{z}_k$ contains both accelerometer-derived angles and gyroscope rates, and $\mathbf{K}_k$ is the Kalman gain weighting measurement trust against prediction trust.

\textbf{Output.} The filter produces orientation angles $[\phi, \theta, \psi]$ that describe body pose:
\begin{itemize}
    \item $\phi$ (roll): Left/right tilt. Increases during lateral falls.
    \item $\theta$ (pitch): Forward/backward tilt. Increases during forward falls.
    \item $\psi$ (yaw): Horizontal heading. \textit{Note:} Yaw is unobservable from gravity alone; we integrate gyroscope angular velocity without accelerometer correction. For our 4.3-second windows, typical MEMS gyroscope bias ($\sim$0.1$^\circ$/s) produces drift of only $\sim$0.4$^\circ$ per window---negligible compared to the large orientation changes ($>$30$^\circ$) characteristic of falls.
\end{itemize}

\textbf{Angle Representation.} We output Euler angles wrapped to $[-\pi, \pi]$ for roll and yaw, and $[-\pi/2, \pi/2]$ for pitch. This standard representation avoids discontinuities for typical fall motions, which rarely involve full rotations. Quaternion output is considered for future work requiring singularity-free representation.

\textbf{Gyroscope Bias.} We do not explicitly model gyroscope bias in the state vector because our 4.3-second windows are short relative to typical MEMS bias drift timescales (minutes to hours). Ablation with 9-dimensional state including bias terms showed negligible improvement (+0.02\% F1), confirming this assumption is reasonable for windowed classification.

Unlike raw gyroscope values that indicate \textit{how fast} the body rotates, orientation angles indicate \textit{what position} the body is in---a semantically richer representation for fall classification.

\textbf{Accelerometer-Derived Reference Angles.} To update the Kalman filter, we compute reference roll and pitch from accelerometer readings:
\begin{align}
\phi_\text{acc} &= \text{atan2}(a_y, \sqrt{a_x^2 + a_z^2}) \\
\theta_\text{acc} &= \text{atan2}(-a_x, a_z)
\end{align}
These formulas follow the smartwatch coordinate convention where +Z points outward from the watch face and +X points toward 3 o'clock. The pitch formula uses atan2($-a_x$, $a_z$) rather than the alternative sqrt-denominator form; both are valid for small tilts, and we empirically validated this choice for our wrist-mounted sensor orientation. These formulas assume the accelerometer measures primarily gravity when stationary; during dynamic motion (falls, vigorous ADLs), linear acceleration contaminates these estimates.

\textbf{Adaptive Measurement Noise.} To handle high-dynamic events where accelerometer-derived orientation becomes unreliable, we scale $R_\text{acc}$ based on acceleration magnitude. At each timestep:
\begin{equation}
R'_\text{acc} = \begin{cases}
R_\text{acc} \cdot \min\left(\left(\frac{\|\mathbf{a}\|}{g}\right)^2, R_\text{max}\right) & \text{if } \|\mathbf{a}\| > \tau \cdot g \\
R_\text{acc} & \text{otherwise}
\end{cases}
\end{equation}
where $\tau = 2.0$ is the activation threshold, $R_\text{max} = 10.0$ is the maximum scale factor, and $R_\text{acc} = 0.05$ is the baseline noise. The scaling is recomputed each timestep based on current $\|\mathbf{a}\|$, automatically resetting when acceleration returns to normal. This reduces filter reliance on accelerometer-derived orientation during impact events.

\textbf{Measurement Model.} The observation vector and measurement matrix are:
\begin{equation}
\mathbf{z}_k = [\phi_\text{acc}, \theta_\text{acc}, \omega_x, \omega_y, \omega_z]^{\T}, \quad
\mathbf{H} = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}
\end{equation}

\textbf{Filter Parameters.} Process and measurement noise covariances:
\begin{itemize}
    \item Process noise: $Q_\phi = 0.005$ (orientation), $Q_{\dot\phi} = 0.01$ (angular rate)
    \item Measurement noise: $R_\text{acc} = 0.05$, $R_\text{gyro} = 0.1$
    \item Initial state: $\mathbf{x}_0 = \mathbf{0}$, $\mathbf{P}_0 = \mathbf{I}_6$
    \item Time step: $\Delta t = 1/30$ s (30 Hz sampling)
\end{itemize}
These values were determined empirically to balance responsiveness against noise filtering.

\textbf{Algorithm Summary.} The complete adaptive Kalman filter operates as follows:
\begin{enumerate}
    \item \textbf{Initialize:} $\hat{\mathbf{x}}_0 = \mathbf{0}$, $\mathbf{P}_0 = \mathbf{I}_6$
    \item \textbf{For each timestep $k$:}
    \begin{enumerate}
        \item Predict: $\hat{\mathbf{x}}_{k|k-1} = \mathbf{F}\hat{\mathbf{x}}_{k-1}$, $\mathbf{P}_{k|k-1} = \mathbf{F}\mathbf{P}_{k-1}\mathbf{F}^{\T} + \mathbf{Q}$
        \item Compute accelerometer-derived angles: $\phi_\text{acc}, \theta_\text{acc}$ from Eq.~(4--5)
        \item Construct measurement: $\mathbf{z}_k = [\phi_\text{acc}, \theta_\text{acc}, \omega_x, \omega_y, \omega_z]^{\T}$
        \item If $\|\mathbf{a}\| > 2.0g$: scale $R_\text{acc} \leftarrow R_\text{acc} \cdot \min((\|\mathbf{a}\|/g)^2, 10)$
        \item Update: $\mathbf{K}_k = \mathbf{P}_{k|k-1}\mathbf{H}^{\T}(\mathbf{H}\mathbf{P}_{k|k-1}\mathbf{H}^{\T} + \mathbf{R})^{-1}$
        \item Correct: $\hat{\mathbf{x}}_k = \hat{\mathbf{x}}_{k|k-1} + \mathbf{K}_k(\mathbf{z}_k - \mathbf{H}\hat{\mathbf{x}}_{k|k-1})$
        \item Output: $[\phi, \theta, \psi] = \hat{\mathbf{x}}_k[0:3]$
    \end{enumerate}
\end{enumerate}

\subsubsection{Feature Representation}

After Kalman fusion, we construct a 7-channel feature vector per timestep:
\begin{equation}
\mathbf{f} = [\underbrace{\text{SMV}, a_x, a_y, a_z}_{\text{acceleration (4ch)}}, \underbrace{\phi, \theta, \psi}_{\text{orientation (3ch)}}]
\end{equation}
where $\text{SMV} = \sqrt{a_x^2 + a_y^2 + a_z^2}$ is the signal magnitude vector. This representation cleanly separates impact dynamics (acceleration) from body pose (orientation).

\subsubsection{Channel-Aware Normalization}

A critical preprocessing detail: we apply \textbf{different normalization strategies} to acceleration and orientation channels:

\begin{itemize}
    \item \textbf{Acceleration channels} (SMV, $a_x$, $a_y$, $a_z$): Standard z-score normalization using training set statistics (mean=0, std=1)
    \item \textbf{Orientation channels} ($\phi$, $\theta$, $\psi$): \textbf{No normalization}---kept in raw radians
\end{itemize}

\textbf{Rationale.} Orientation angles have inherent physical meaning: $\phi = 0$ indicates the wrist is level, $\theta = \pm\pi/2$ indicates vertical orientation. Z-score normalization would destroy this semantic structure by centering around the training mean (which varies by subject posture habits). By preserving raw radians, the neural network can learn physically meaningful patterns (e.g., ``pitch angle exceeds 1.0 radian during forward falls'').

\textbf{Coordinate frame.} Orientation angles are computed in the \textit{device frame} (sensor-fixed coordinates) without explicit body-frame alignment. This is appropriate because: (1) all sensors are wrist-mounted with consistent orientation relative to the watch band, and (2) fall signatures (rapid tilt changes, final horizontal orientation) are identifiable in any consistent frame. No magnetometer-based heading correction is applied; yaw represents gyroscope-integrated heading relative to initial orientation.

This ``channel-aware'' normalization contributes +1.65\% F1 improvement over unified z-score normalization (91.10\% vs. 89.45\%), as detailed in Appendix Table~\ref{tab:norm_ablation}.

%-------------------------------------------------------------------
% 2.3 DUAL-STREAM ARCHITECTURE
%-------------------------------------------------------------------
\subsection{Dual-Stream Input Projection Architecture}
\label{subsec:architecture}

With our 7-channel Kalman-fused representation, we now describe the neural architecture. A key design question is: \textit{how should multi-modal input be processed?} We compare architectures that differ in whether accelerometer and orientation channels share or have separate early processing layers. Figure~\ref{fig:architecture} presents our dual-stream architecture.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{arch_dual_stream_kalman.png}
\caption{\textbf{Dual-stream Kalman transformer architecture.} \textbf{Left:} The accelerometer stream (4 channels: SMV, $a_x$, $a_y$, $a_z$) and orientation stream (3 channels: $\phi$, $\theta$, $\psi$) are processed through \textit{separate} Conv1D projection layers (kernel size 8, stride 1, same padding), each with dedicated batch normalization and dropout. This separation prevents noisy orientation features from corrupting acceleration representations. \textbf{Center:} Projected features are concatenated (64 dimensions total, 32:32 split) and processed by a 2-layer transformer encoder with 4 attention heads, enabling cross-modal temporal pattern learning. \textbf{Right:} Squeeze-and-Excitation (SE) attention recalibrates channel importance, followed by Temporal Attention Pooling (TAP) which focuses on discriminative timesteps (typically the impact phase). Final classification uses a single linear layer with sigmoid activation. Total parameters: $\sim$73K.}
\label{fig:architecture}
\end{figure}

\subsubsection{What We Mean by ``Dual-Stream''}

We use ``dual-stream'' to describe architectures with \textbf{separate input projection layers} for each modality, followed by a shared transformer encoder. This is distinct from fully-separate dual-backbone architectures where each modality has its own transformer. Our design choice is deliberate:

\begin{itemize}
    \item \textbf{Separate input projections}: Each modality (acceleration, orientation) has dedicated Conv1D + BatchNorm layers. This prevents cross-modal interference during the critical initial feature extraction stage.
    \item \textbf{Shared transformer}: After projection, features are concatenated and processed by a single transformer. This enables cross-modal attention---the model can learn relationships \textit{between} acceleration and orientation patterns.
\end{itemize}

The separation occurs at the \textit{input encoding stage}, where the risk of noise contamination is highest. The shared transformer then learns joint temporal representations.

\subsubsection{Architecture Variants}

We systematically compare four configurations along two axes:
\begin{itemize}
    \item \textbf{Input type}: Raw (6ch: acc + gyro) vs. Kalman (7ch: acc + orientation)
    \item \textbf{Input projection}: Single (shared Conv1D) vs. Dual (separate Conv1D per modality)
\end{itemize}

\begin{table}[H]
\caption{Architecture comparison: single-projection vs. dual-projection. The key difference is whether modalities share convolutional weights (single) or have dedicated weights (dual). Our method (d) combines dual-projection with Kalman-fused orientation input. Raw configurations use 6 channels (acc + gyro, no SMV); Kalman configurations use 7 channels (SMV + acc + orientation).}
\label{tab:architecture_comparison}
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Architecture} & \textbf{Input Type} & \textbf{Projection} & \textbf{Key Property} \\
\midrule
\multicolumn{4}{l}{\textit{Single-Projection (shared input encoding):}} \\
(a) Single + Raw & 6ch ($a_{xyz}$ + $\omega_{xyz}$) & Shared Conv1D & Noisy gyro corrupts acc features \\
(b) Single + Kalman & 7ch (SMV + $a_{xyz}$ + $\phi\theta\psi$) & Shared Conv1D & Cleaner, but features still mixed \\
\midrule
\multicolumn{4}{l}{\textit{Dual-Projection (separate input encoding):}} \\
(c) Dual + Raw & 3ch + 3ch & Separate Conv1D & Acc isolated, but gyro still noisy \\
\textbf{(d) Dual + Kalman} & 4ch + 3ch & Separate Conv1D & \textbf{Clean isolation + quality input} \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{Architecture (d)} represents our proposed approach: Kalman fusion first transforms noisy gyroscope signals into stable orientation angles, then dual-projection processes each modality through dedicated Conv1D layers (acc: 4ch$\rightarrow$32dim, ori: 3ch$\rightarrow$32dim) before concatenation and transformer processing.

\subsubsection{The Architectural Difference: Where Separation Matters}

The key question is: \textit{at what stage should modalities be combined?} We compare two approaches:

\textbf{Single-Projection (``Single-Stream'') Architecture}:
\begin{equation}
\mathbf{h} = \text{Conv1D}([\mathbf{x}_\text{acc}; \mathbf{x}_\text{gyro}]) \in \mathbb{R}^{T \times 64}
\end{equation}
\begin{itemize}
    \item All input channels are concatenated \textit{before} any processing.
    \item A single Conv1D layer with shared weights operates across all channels.
    \item \textbf{The problem}: Convolutional filters must simultaneously extract features from clean accelerometer signals and noisy gyroscope signals. The learned filters compromise between modalities. Mathematically, gradients from noisy channels affect all filter weights.
\end{itemize}

\textbf{Dual-Projection (``Dual-Stream'') Architecture}:
\begin{equation}
\mathbf{h} = [\underbrace{\text{Conv1D}_\text{acc}(\mathbf{x}_\text{acc})}_{\text{32 dim}}; \underbrace{\text{Conv1D}_\text{ori}(\mathbf{x}_\text{ori})}_{\text{32 dim}}] \in \mathbb{R}^{T \times 64}
\end{equation}
\begin{itemize}
    \item Input is split by modality \textit{before} convolution.
    \item Each modality has dedicated Conv1D weights optimized for its signal characteristics.
    \item Concatenation occurs \textit{after} initial projection, feeding into the shared transformer.
    \item \textbf{The benefit}: Accelerometer features are extracted by filters that never see gyroscope noise. Each projection layer can specialize: $\text{Conv1D}_\text{acc}$ learns acceleration patterns, $\text{Conv1D}_\text{ori}$ learns orientation patterns.
\end{itemize}

\textbf{Important clarification}: Both architectures use the \textit{same} transformer encoder after projection. The separation is specifically at the input encoding stage---where noisy signals would otherwise contaminate clean features through shared convolutional weights. The shared transformer is beneficial because it enables cross-modal attention (relating acceleration patterns to orientation patterns).

\subsubsection{Why Dual-Stream Matters More with Kalman Fusion}

An important interaction effect emerges: dual-stream processing provides larger benefits with Kalman input than with raw input.

With \textbf{raw gyroscope input}, the gyroscope stream is inherently noisy. Even with separate processing, the gyroscope encoder extracts limited useful information, constraining the potential benefit of dual-stream design.

With \textbf{Kalman-fused orientation input}, the orientation stream contains high-quality, semantically meaningful features (body pose angles). Dual-stream processing allows the model to fully leverage these features with dedicated capacity, while maintaining clean accelerometer representations.

This synergy motivates our combined approach: Kalman fusion produces high-quality orientation features, and dual-stream processing ensures these features are not corrupted during early network layers.

\subsubsection{Stream Encoder Details}

Each stream encoder consists of:
\begin{align}
\mathbf{h}_\text{acc} &= \text{Dropout}_{0.1}(\text{SiLU}(\text{BN}(\text{Conv1D}_{k=8}(\mathbf{x}_\text{acc})))) \in \mathbb{R}^{T \times 32} \\
\mathbf{h}_\text{ori} &= \text{Dropout}_{0.15}(\text{SiLU}(\text{BN}(\text{Conv1D}_{k=8}(\mathbf{x}_\text{ori})))) \in \mathbb{R}^{T \times 32}
\end{align}

Design choices:
\begin{itemize}
    \item \textbf{Kernel size $k=8$}: Captures ~0.27s of temporal context, sufficient for local motion patterns.
    \item \textbf{Equal dimensions (32 each)}: Balanced capacity, appropriate since Kalman fusion produces orientation features of comparable discriminative value to acceleration.
    \item \textbf{Higher dropout for orientation (15\% vs 10\%)}: Slight additional regularization for the orientation stream, which has fewer input channels.
\end{itemize}

\subsubsection{Transformer Encoder}

After concatenation and layer normalization, the fused representation is processed by a standard transformer encoder:
\begin{equation}
\mathbf{h}_\text{enc} = \text{TransformerEncoder}(\text{LayerNorm}([\mathbf{h}_\text{acc}; \mathbf{h}_\text{ori}]))
\end{equation}
Configuration: $d_\text{model}=64$, $n_\text{heads}=4$, $d_\text{ff}=128$, $n_\text{layers}=2$, pre-norm residual connections.

Multi-head self-attention enables the model to relate distant timesteps---for example, connecting pre-fall motion patterns to impact events occurring seconds later.

%-------------------------------------------------------------------
% 2.4 ATTENTION MECHANISMS
%-------------------------------------------------------------------
\subsection{Attention Mechanisms}
\label{subsec:attention}

We augment the transformer with two attention mechanisms that provide complementary benefits.

\subsubsection{Squeeze-and-Excitation (SE) Channel Attention}

SE attention~\cite{hu2018squeeze} recalibrates channel importance by learning which feature dimensions are most discriminative:
\begin{align}
\mathbf{s} &= \text{GlobalAvgPool}(\mathbf{h}_\text{enc}) \in \mathbb{R}^{64} \\
\mathbf{e} &= \sigma(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \cdot \mathbf{s})) \in \mathbb{R}^{64} \\
\hat{\mathbf{h}} &= \mathbf{e} \odot \mathbf{h}_\text{enc}
\end{align}
The bottleneck ($r=4$) prevents overfitting while enabling channel interaction learning.

\subsubsection{Temporal Attention Pooling (TAP)}

Rather than mean-pooling, TAP learns to focus on discriminative timesteps:
\begin{align}
\alpha_t &= \frac{\exp(\mathbf{w}^{\T} \tanh(\mathbf{W}_a \mathbf{h}_t))}{\sum_{t'} \exp(\mathbf{w}^{\T} \tanh(\mathbf{W}_a \mathbf{h}_{t'}))} \\
\mathbf{c} &= \sum_t \alpha_t \hat{\mathbf{h}}_t
\end{align}
TAP typically assigns high attention to the impact phase of falls, where acceleration spikes and orientation changes are most pronounced.

%-------------------------------------------------------------------
% 2.5 TRAINING AND EVALUATION
%-------------------------------------------------------------------
\subsection{Training Configuration}
\label{subsec:training}

\textbf{Loss Function.} We use Focal Loss~\cite{lin2017focal} with $\alpha=0.75$, $\gamma=2.0$ to handle class imbalance, down-weighting well-classified examples to focus on hard cases. These hyperparameters were chosen via validation once and held fixed for all experiments.

\textbf{Optimization.} AdamW optimizer with learning rate $10^{-3}$, weight decay $5 \times 10^{-4}$, batch size 64, trained for 80 epochs.

\textbf{Regularization.} Dropout ($p=0.5$) before classification; batch normalization in stream encoders.

\subsection{Evaluation Protocol}
\label{subsec:evaluation}

\textbf{Cross-Validation.} 21-fold Leave-One-Subject-Out CV evaluates generalization to unseen individuals. In each fold, one subject's data serves as test set, subjects 48 and 57 provide validation data for early stopping, and remaining subjects form the training set.

\textbf{Metrics.} F1 score serves as the primary metric due to class imbalance (approximately 65\% ADL, 35\% fall windows). We also report precision, recall, accuracy, and AUC-ROC.

\textbf{Confidence Intervals.} 95\% CIs are computed over the 21 fold-level F1 scores (not window-level predictions), ensuring statistical independence between samples. Standard error is $\text{SE} = \sigma / \sqrt{21}$ where $\sigma$ is the standard deviation across folds.

\textbf{Statistical Testing.} Paired t-tests ($n=21$ folds) assess significance of performance differences between model variants, with Holm-Bonferroni correction for multiple comparisons when comparing more than two configurations.
